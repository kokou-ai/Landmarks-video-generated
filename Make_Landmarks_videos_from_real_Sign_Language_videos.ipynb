{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zKMryIkKpCIi",
        "cIveyrnKL85C",
        "I2DIDYl4R7T9",
        "jrave0M2L72F",
        "Feo7cmrbrnPe",
        "4MVkz5fG8tkx",
        "G7eGeQFsJtER"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intall packages"
      ],
      "metadata": {
        "id": "zKMryIkKpCIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-rNIpLQCo4gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe tensorflow transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCQS7CKQo_p-",
        "outputId": "b7abc35a-c0fe-4aef-ca76-596e2cc9744c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.15)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.4)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIveyrnKL85C"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RNkdtEeDMTnx"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "from typing import List, Optional, Tuple, Union, Mapping\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rEsgMfTDleNR"
      },
      "outputs": [],
      "source": [
        "#Define workdir\n",
        "workdir = \"/content/drive/MyDrive/Sign_lang\"\n",
        "os.chdir(workdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmw2fDTZlfC-",
        "outputId": "df7014c3-4f4b-4e73-951f-6652b9de5326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 3d_landmarks_bgr.png\t\t\t\t\t\t\t       video.mp4\n",
            " 3d_landmarks.png\t\t\t\t\t\t\t       WLASL_v0.3.json\n",
            " computer_landmarks.avi\t\t\t\t\t\t\t       word_classes.txt\n",
            " computer_landmarks.mp4\t\t\t\t\t\t\t       X_test.npy\n",
            "'Make Landmarks videos from real  Sign Language videos.ipynb'\t\t       X_test_video.npy\n",
            "'Mediapipe_data Extract.ipynb'\t\t\t\t\t\t       X_train.npy\n",
            " mygeneratedvideo.avi\t\t\t\t\t\t\t       X_train_video.npy\n",
            " sign_cnn_lstm_model.h5\t\t\t\t\t\t\t       X_val.npy\n",
            "'Sign Lang Machine Translation With Visual Transformers(TimeSformer) .ipynb'   X_val_video.npy\n",
            "'SignLang with CNN- LSTM.ipynb'\t\t\t\t\t\t       y_test.npy\n",
            " video0.avi\t\t\t\t\t\t\t\t       y_test_video.npy\n",
            " video1.avi\t\t\t\t\t\t\t\t       y_train.npy\n",
            " video2.avi\t\t\t\t\t\t\t\t       y_train_video.npy\n",
            " video2.mp4\t\t\t\t\t\t\t\t       y_val.npy\n",
            " video3.avi\t\t\t\t\t\t\t\t       y_val_video.npy\n",
            " video_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2DIDYl4R7T9"
      },
      "source": [
        "# Mediapipe video to matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z3WThhrUWFtg"
      },
      "outputs": [],
      "source": [
        "mp_holistic = mp.solutions.holistic\n",
        "mp_draw = mp.solutions.drawing_styles\n",
        "\n",
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results\n",
        "\n",
        "def preprocess(array):\n",
        "  mean = np.mean(array)\n",
        "  std = np.std(array)\n",
        "  return (array-mean)/std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drawing\n",
        "@dataclass\n",
        "class DrawingSpec:\n",
        "    color: tuple\n",
        "    thickness: int\n",
        "    circle_radius: int\n",
        "\n",
        "WHITE_COLOR = (255, 255, 255)\n",
        "RED_COLOR =  WHITE_COLOR #(0, 0, 255)\n",
        "_BGR_CHANNELS = 3\n",
        "\n",
        "def draw_face_landmarks(\n",
        "    image: np.ndarray,\n",
        "    landmarks: np.ndarray,  # Replace by `landmark_list`\n",
        "    connections: Optional[List[Tuple[int, int]]] = None,\n",
        "    landmark_drawing_spec: Optional[\n",
        "        Union[DrawingSpec, Mapping[int, DrawingSpec]]\n",
        "    ] = DrawingSpec(color=RED_COLOR, thickness=2, circle_radius=5),\n",
        "    connection_drawing_spec: Union[\n",
        "        DrawingSpec, Mapping[Tuple[int, int], DrawingSpec]\n",
        "    ] = DrawingSpec(color=RED_COLOR, thickness=1, circle_radius=5),\n",
        "    is_drawing_landmarks: bool = True,\n",
        "):\n",
        "    \"\"\"Draws the face landmarks and the connections on the image.\n",
        "\n",
        "    Args:\n",
        "        image: A three-channel BGR image represented as numpy ndarray.\n",
        "        landmarks: A numpy array of face landmarks with shape (468, 3).\n",
        "        connections: A list of landmark index tuples that specifies how landmarks to\n",
        "                     be connected in the drawing.\n",
        "        landmark_drawing_spec: Either a DrawingSpec object or a mapping from face\n",
        "                               landmarks to the DrawingSpecs that specifies the landmarks' drawing\n",
        "                               settings such as color, line thickness, and circle radius.\n",
        "        connection_drawing_spec: Either a DrawingSpec object or a mapping from face\n",
        "                                  connections to the DrawingSpecs that specifies the connections' drawing\n",
        "                                  settings such as color and line thickness.\n",
        "        is_drawing_landmarks: Whether to draw landmarks. If set false, skip drawing\n",
        "                              landmarks, only contours will be drawn.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input image is not three-channel BGR.\n",
        "    \"\"\"\n",
        "    if image.shape[2] != _BGR_CHANNELS:\n",
        "        raise ValueError('Input image must contain three channel BGR data.')\n",
        "\n",
        "    image_rows, image_cols, _ = image.shape\n",
        "    idx_to_coordinates = {}\n",
        "\n",
        "    for idx, (x, y, z) in enumerate(landmarks):\n",
        "        # Convert normalized coordinates to pixel coordinates\n",
        "        landmark_px = (int(x * image_cols), int(y * image_rows))  # Here z is ignored for 2D display\n",
        "        idx_to_coordinates[idx] = landmark_px\n",
        "\n",
        "    if connections:\n",
        "        num_landmarks = len(landmarks)\n",
        "        # Draw the connections if the start and end landmarks are both visible\n",
        "        for connection in connections:\n",
        "            start_idx, end_idx = connection\n",
        "            if not (0 <= start_idx < num_landmarks and 0 <= end_idx < num_landmarks):\n",
        "                raise ValueError(f'Landmark index is out of range. Invalid connection '\n",
        "                                 f'from landmark #{start_idx} to landmark #{end_idx}.')\n",
        "            if start_idx in idx_to_coordinates and end_idx in idx_to_coordinates:\n",
        "                drawing_spec = connection_drawing_spec if not isinstance(\n",
        "                    connection_drawing_spec, Mapping) else connection_drawing_spec.get(connection, DrawingSpec(color=RED_COLOR, thickness=1))\n",
        "                cv2.line(image, idx_to_coordinates[start_idx], idx_to_coordinates[end_idx],\n",
        "                         drawing_spec.color, drawing_spec.thickness)\n",
        "\n",
        "    if is_drawing_landmarks and landmark_drawing_spec:\n",
        "        for idx, landmark_px in idx_to_coordinates.items():\n",
        "            drawing_spec = landmark_drawing_spec if not isinstance(\n",
        "                landmark_drawing_spec, Mapping) else landmark_drawing_spec.get(idx, DrawingSpec(color=RED_COLOR, thickness=2, circle_radius=5))\n",
        "            # Draw the landmark points\n",
        "            circle_border_radius = max(drawing_spec.circle_radius + 1,\n",
        "                                       int(drawing_spec.circle_radius * 1.2))\n",
        "            '''cv2.circle(image, landmark_px, circle_border_radius, WHITE_COLOR,\n",
        "                       drawing_spec.thickness)\n",
        "            cv2.circle(image, landmark_px, drawing_spec.circle_radius,\n",
        "                       drawing_spec.color, drawing_spec.thickness)'''\n"
      ],
      "metadata": {
        "id": "MsuQoqYt5qKT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1ctyFvuX6N3u"
      },
      "outputs": [],
      "source": [
        "# mp_holistic.FACEMESH_CONTOURS\n",
        "# Custom colors\n",
        "left_hand_spec = DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=5)  # Green for left hand\n",
        "right_hand_spec = DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=5)  # Blue for right hand\n",
        "\n",
        "def display_landmarks(frame, face, pose, lh, rh):\n",
        "    # Draw face landmarks in video\n",
        "    draw_face_landmarks(image=frame, landmarks=face, connections=mp_holistic.FACEMESH_CONTOURS)\n",
        "\n",
        "    # Draw body landmarks\n",
        "    draw_face_landmarks(image=frame, landmarks=pose, connections=mp_holistic.POSE_CONNECTIONS)\n",
        "\n",
        "    # Draw left hand landmarks (with green color)\n",
        "    draw_face_landmarks(image=frame, landmarks=lh, connections=mp_holistic.HAND_CONNECTIONS,\n",
        "                        landmark_drawing_spec=left_hand_spec,\n",
        "                        connection_drawing_spec=left_hand_spec)\n",
        "\n",
        "    # Draw right hand landmarks (with blue color)\n",
        "    draw_face_landmarks(image=frame, landmarks=rh, connections=mp_holistic.HAND_CONNECTIONS,\n",
        "                        landmark_drawing_spec=right_hand_spec,\n",
        "                        connection_drawing_spec=right_hand_spec)\n",
        "\n",
        "    # Display the result\n",
        "    # cv2_imshow(frame)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d3syta7RbmKb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "holistic=mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "\n",
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    all = []\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "    else:\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        print(f\"Frames per second (FPS): {fps}\")\n",
        "        print(f\"Total number of frames: {frame_count}\")\n",
        "\n",
        "        frame_index = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                # Fin de vidÃ©o ou erreur\n",
        "                print(\"No more frames or error occurred.\")\n",
        "                break\n",
        "\n",
        "            if frame is None or frame.size == 0:\n",
        "                # Frame corrompue\n",
        "                print(\"Error: Frame is invalid or empty. Trying to continue...\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            # Processer l'image avec MediaPipe Holistic\n",
        "            frame, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "            #draw_styled_landmarks(frame, results)\n",
        "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros((33, 3))\n",
        "            face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros((468, 3))\n",
        "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21, 3))\n",
        "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21, 3))\n",
        "\n",
        "\n",
        "            out = np.concatenate([face, pose, lh, rh])\n",
        "            all.append(np.array(out))\n",
        "\n",
        "\n",
        "\n",
        "            frame_index += 1\n",
        "            if cv2.waitKey(int(1000 / fps)) & 0xFF == ord('q'):\n",
        "                break\n",
        "        cap.release()\n",
        "\n",
        "        padding = 60\n",
        "        all = all[:padding] if len(all) > padding else all+list(np.zeros((padding-len(all), all[0].shape[0], all[0].shape[1] )))\n",
        "        all= np.array(all)\n",
        "\n",
        "        #Preprocessing\n",
        "        #all = preprocess(all)\n",
        "\n",
        "        return all\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrave0M2L72F"
      },
      "source": [
        "# Display the landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJKS_2AzZ2B5",
        "outputId": "9bf0e8d8-417a-4acf-fff6-f5d01eb3fa4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames per second (FPS): 25.0\n",
            "Total number of frames: 58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No more frames or error occurred.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 543, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Test\n",
        "video_path = \"video_data/train/book/07068.mp4\"\n",
        "out = process_video(video_path)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4NXHpSGqRSb",
        "outputId": "efd361ee-4142-4f4a-d745-becffcad8b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 543) (1, 543) (1, 543)\n"
          ]
        }
      ],
      "source": [
        "x = out[:1, :, 0]\n",
        "y = out[:1,:, 1]\n",
        "z = out[:1, :, 2]\n",
        "print(x.shape, y.shape,z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_AEp4Fdyr0e"
      },
      "source": [
        " ## Draw landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VLmDNcZG5EMp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6f9279c6-4a22-473d-e6de-c113a0b88cc1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=255x255>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD/CAIAAACxapedAAAFA0lEQVR4nO3dzXqiShiFUXKevv9b5gxMEzsillj8fXutUQ+MLfpSFgXqMAzjMIwDRJI+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACn93X0A8gyjmPjLb++vDSb8xRv7r749qZvf2Uf2JQndytT9J8U3OVOYFftM5z2O+x+n9DfdpnaATi1rQO1A3BS+6RpB+jlv6MfAHB9ew7Jhv8u/hz9ACp71qjly5NQfx/jON43PZ2rml2wf3Yma3lEt890p/7+bnvCbYV+SrbljO9y387+duep7ONx7F/I9GXHC1Om6Z6X/wvYT5fD0PGvlv/Fge/nzHw2136V28ux3GDfl/o39DjDaZm7Pw7qot+I+jf3WPPClH06Yh7uone8uxH1b2u58tnbT3vLr93GYW536t/Kulg/XCyCAzyO0+/+SeNfTTez5sNZrFuLXLHPTLdUP2exusVns/zt/kcmrnA+BUe0XNgnI/GKvzXwd2HsP55R/yjqJ5f6gY/5ZCO5ditS+r2Y+ZBL/eRSP9DDWzPyddfqmPR35ArnvbVf2eYs2NbUv6vGkVv3+zDv7+n+k1kLt1kdt4vh+jL27+rxM7u/mNbvyUDSX8sIveIrPg38XMMWQ7i3Ba5hi9/t6nuHsKG+vaqfi+mVrPS5pM/DlT4XtvNHfuFcXNJDurf2AelT0HLWL3/Ago6cOzyA6zoBAAAAAAAAAAAAAAAAAAAAAMrx7TE/DvwOKV/jcwi/2/WPoyo8asez1/Et7fsD07b3kV8s/SaFQOqPFr7Pqz+U3z8d1E8y9f+YfkidEOofBtOAVOonl/qjhU/21E8u9SdynHOjfinkUj+51E8u9adLXvZRP7nUD6l+veknzAEetzFhq2cZ+/+RPAkOpH5y93n1k0v95Iqu3zUO4aLrJ5z6s3i7u6d+hiF12Uf95Mqt3xyA3PpB/eRSfxCTPb4tLHFUXf14uV1VN/yZ0LHfKMgQWz8M6ieZ+smlfn5kXu+QJXPpo3GjSm77M8Z+cqmfXOonl/ohRstRXckjv/aNKrn5s4z9ECNz4e+tzSm27QuM/eRSP7my6ndhM/ey6qdFztU+6ieX+sml/voc7TyjfggQe8Zn3bZUegaeMfaTS/3kUj+51D8v54wP9a1IuUb9q7eixuYvM/YzL+HdT/3kUj+51E+uiPpd6MKsiPohV+yFLp9vQoEnYYGxn1zqJ5f6yaX+pxJOdoZTP7nUTy71l+Uc30vqJ5f6yaV+cqmfXOrnqfLHzeqHuj45X3vdc71dHvl1N7+RsZ9c6ieX+pe40K029ZNL/eRSP7nUX1D5s1S9qJ9c6ieX+smlfnKpn1zqr8aCTzv1k0v95FL/Cy50K0z9zEs4flA/udRPLvWXkjBd6Uj95FI/udRPLvWTS/3kUj+51F+H5c53Fa9fECwoXn8XLnSrSv3kUj+51E8u9ZNL/UVY3Vrhz9EP4DIal30keCHqb9WYdfe1UbvTdtTfWfdYDznVEDKPUv/ZHfWek8BRL7nUTy71k0v95Kpc/+1A0OEgz1SuH5apn1xl6w85XwMz7if9Xab+Zz5+6PvYxnE888Z2VHPsN/DTomb9Ex/Jfdft6Qp53orXfxPyWvKugvWb9qyW9tQVrH9W1ItKo5T6eSlt4B/q1R/4ErJatfqHv8vVDnPfMo0a0z8SlgqqfbbrfuAv/+LxoWr13zMFahQ7Xfwf/UmmHaExPnMAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#make Black background\n",
        "\n",
        "height = 255\n",
        "width = 255\n",
        "for i in range(1):\n",
        "  img = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "  matrix = out[i]\n",
        "  face = matrix[:468]\n",
        "  pose = matrix[468: 501]\n",
        "  lh = matrix[501: 522]\n",
        "  rh = matrix[522: ]\n",
        "  display_landmarks(img, face, pose, lh, rh)\n",
        "  cv2_imshow(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Video"
      ],
      "metadata": {
        "id": "SvtN0dFo30_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwF0QYcIKctO",
        "outputId": "a87192e2-9d5b-440b-9890-131f2f81d3fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 543, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Video Generating function\n",
        "def generate_video(out, video_name, height, width):\n",
        "  fps = 30\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec pour MP4\n",
        "  video = cv2.VideoWriter(video_name, fourcc, fps, (width, height))\n",
        "  height, width, layers = (height, width, 3)\n",
        "\n",
        "\n",
        "\t#for image in out:\n",
        "  for i in range(60):\n",
        "    img = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "    matrix = out[i]\n",
        "    face = matrix[:468]\n",
        "    pose = matrix[468: 501]\n",
        "    lh = matrix[501: 522]\n",
        "    rh = matrix[522: ]\n",
        "    display_landmarks(img, face, pose, lh, rh)\n",
        "    #cv2_imshow(img)\n",
        "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    video.write(img)\n",
        "  # Deallocating memories taken for window creation\n",
        "  cv2.destroyAllWindows()\n",
        "  video.release() # releasing the video generated\n",
        "\n",
        "\n",
        "# Calling the generate_video function\n",
        "#generate_video(out, video_name, height, width)\n"
      ],
      "metadata": {
        "id": "Ht-tU2an32jW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Video with Landmarks"
      ],
      "metadata": {
        "id": "Feo7cmrbrnPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic=mp_holistic.Holistic(min_detection_confidence=0.3, min_tracking_confidence=0.5)\n",
        "video_path = \"video_data/val/computer/12335.mp4\"\n",
        "video_landmarks = process_video(video_path)\n",
        "print(video_landmarks.shape)\n",
        "generate_video(video_landmarks, \"computer_landmarks.mp4\", 480, 640)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWS10Lkdr-bx",
        "outputId": "b21632e9-5bcd-4ed1-8ccf-fa4d364451d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames per second (FPS): 25.0\n",
            "Total number of frames: 64\n",
            "No more frames or error occurred.\n",
            "(60, 543, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "XoIKIfWO1_4s",
        "outputId": "b259c0ff-1d9e-4025-81f9-cd4e1b4e418d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 3d_landmarks_bgr.png\t\t\t\t\t\t\t       video.mp4\n",
            " 3d_landmarks.png\t\t\t\t\t\t\t       WLASL_v0.3.json\n",
            " computer_landmarks.avi\t\t\t\t\t\t\t       word_classes.txt\n",
            " computer_landmarks.mp4\t\t\t\t\t\t\t       X_test.npy\n",
            "'Mediapipe_data Extract.ipynb'\t\t\t\t\t\t       X_test_video.npy\n",
            " mygeneratedvideo.avi\t\t\t\t\t\t\t       X_train.npy\n",
            " sign_cnn_lstm_model.h5\t\t\t\t\t\t\t       X_train_video.npy\n",
            "'Sign Lang Machine Translation With Visual Transformers(TimeSformer) .ipynb'   X_val.npy\n",
            "'SignLang with CNN- LSTM.ipynb'\t\t\t\t\t\t       X_val_video.npy\n",
            " video0.avi\t\t\t\t\t\t\t\t       y_test.npy\n",
            " video1.avi\t\t\t\t\t\t\t\t       y_test_video.npy\n",
            " video2.avi\t\t\t\t\t\t\t\t       y_train.npy\n",
            " video2.mp4\t\t\t\t\t\t\t\t       y_train_video.npy\n",
            " video3.avi\t\t\t\t\t\t\t\t       y_val.npy\n",
            " video_data\t\t\t\t\t\t\t\t       y_val_video.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MVkz5fG8tkx"
      },
      "source": [
        "# Make Landmarks dataset\n",
        "\n",
        "\n",
        "* For each video, we have a matrix of (60, 543, 3); we will create a dataset divided into train, test, and validation sets.\n",
        "\n",
        "* Each data slice (train, test, val) contains folders named after the labels.\n",
        "\n",
        "* We will save these data into folders in .npy format, which will be used later by the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7eGeQFsJtER"
      },
      "source": [
        "## Create folders and moove videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS1YSVmg_nXk",
        "outputId": "301eb339-b526-495a-dca2-34b4f0d73411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 3d_landmarks_bgr.png\t\t\t\t\t\t\t       WLASL_v0.3.json\n",
            " 3d_landmarks.png\t\t\t\t\t\t\t       word_classes.txt\n",
            "'Mediapipe_data Extract.ipynb'\t\t\t\t\t\t       X_test.npy\n",
            " mygeneratedvideo.avi\t\t\t\t\t\t\t       X_test_video.npy\n",
            " sign_cnn_lstm_model.h5\t\t\t\t\t\t\t       X_train.npy\n",
            "'Sign Lang Machine Translation With Visual Transformers(TimeSformer) .ipynb'   X_train_video.npy\n",
            "'SignLang with CNN- LSTM.ipynb'\t\t\t\t\t\t       X_val.npy\n",
            " video0.avi\t\t\t\t\t\t\t\t       X_val_video.npy\n",
            " video1.avi\t\t\t\t\t\t\t\t       y_test.npy\n",
            " video2.avi\t\t\t\t\t\t\t\t       y_test_video.npy\n",
            " video2.mp4\t\t\t\t\t\t\t\t       y_train.npy\n",
            " video3.avi\t\t\t\t\t\t\t\t       y_train_video.npy\n",
            " video_data\t\t\t\t\t\t\t\t       y_val.npy\n",
            " video.mp4\t\t\t\t\t\t\t\t       y_val_video.npy\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcgrFcAD_A5b",
        "outputId": "0222c36b-7c6c-44db-8709-b58a3b5a067d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 2000 differents words\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join(workdir, \"word_classes.txt\")) as f:\n",
        "  liste = f.readlines()\n",
        "liste = [i.split(\"\\t\")[-1].split(\"\\n\")[0] for i in liste]\n",
        "liste = [i for i in liste if i!= '']\n",
        "print(f\"We have {len(liste)} differents words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ti5rPqlBeNa"
      },
      "outputs": [],
      "source": [
        "# We'll use just the 5 st words for test purposes\n",
        "classes_list = liste[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYW9ThdAChld",
        "outputId": "438a68a8-52c2-4c6b-8f69-156171a0405d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'gloss': 'book', 'instances': [{'bbox': [385, 37, 885, 720], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 0, 'signer_id': 118, 'source': 'aslbrick', 'split': 'train', 'url': 'http://aslbricks.org/New/ASL-Videos/book.mp4', 'variation_id': 0, 'video_id': '69241'}, {'bbox': [190, 25, 489, 370], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 1, 'signer_id': 90, 'source': 'aslsignbank', 'split': 'train', 'url': 'https://aslsignbank.haskins.yale.edu/dictionary/protected_media/glossvideo/ASL/BO/BOOK-418.mp4', 'variation_id': 0, 'video_id': '65225'}, {'bbox': [262, 1, 652, 480], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 2, 'signer_id': 110, 'source': 'valencia-asl', 'split': 'train', 'url': 'https://www.youtube.com/watch?v=0UsjUE-TXns', 'variation_id': 0, 'video_id': '68011'}, {'bbox': [123, 19, 516, 358], 'fps': 25, 'frame_end': 60, 'frame_start': 1, 'instance_id': 3, 'signer_id': 113, 'source': 'lillybauer', 'split': 'train', 'url': 'https://www.youtube.com/watch?v=1QOYOZ3g-aY', 'variation_id': 0, 'video_id': '68208'}, {'bbox': [95, 0, 1180, 720], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 4, 'signer_id': 109, 'source': 'valencia-asl', 'split': 'train', 'url': 'https://www.youtube.com/watch?v=aGtIHKEdCds', 'variation_id': 0, 'video_id': '68012'}, {'bbox': [110, 25, 274, 240], 'fps': 25, 'frame_end': 2249, 'frame_start': 2150, 'instance_id': 5, 'signer_id': 121, 'source': 'northtexas', 'split': 'val', 'url': 'https://www.youtube.com/watch?v=hjS0dQDgbjo', 'variation_id': 0, 'video_id': '70212'}, {'bbox': [153, 38, 395, 360], 'fps': 25, 'frame_end': 3852, 'frame_start': 3732, 'instance_id': 6, 'signer_id': 121, 'source': 'northtexas', 'split': 'train', 'url': 'https://www.youtube.com/watch?v=WGfiiDgrq1I', 'variation_id': 0, 'video_id': '70266'}, {'bbox': [16, 2, 235, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 7, 'signer_id': 49, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_english_grammar.swf', 'variation_id': 0, 'video_id': '07085'}, {'bbox': [16, 4, 239, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 8, 'signer_id': 49, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_geography.swf', 'variation_id': 0, 'video_id': '07086'}, {'bbox': [8, 1, 253, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 9, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_geometry.swf', 'variation_id': 0, 'video_id': '07087'}, {'bbox': [462, 44, 949, 720], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 10, 'signer_id': 31, 'source': 'signschool', 'split': 'train', 'url': 'https://signstock.blob.core.windows.net/signschool/videos/SignSchool%20Book.mp4', 'variation_id': 0, 'video_id': '07069'}, {'bbox': [29, 4, 227, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 11, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_history.swf', 'variation_id': 0, 'video_id': '07088'}, {'bbox': [22, 0, 226, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 12, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_law.swf', 'variation_id': 0, 'video_id': '07089'}, {'bbox': [34, 2, 229, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 13, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_literature.swf', 'variation_id': 0, 'video_id': '07090'}, {'bbox': [23, 1, 226, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 14, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_math.swf', 'variation_id': 0, 'video_id': '07091'}, {'bbox': [31, 4, 220, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 15, 'signer_id': 49, 'source': 'aslpro', 'split': 'test', 'url': 'http://www.aslpro.com/main/b/book_medicine.swf', 'variation_id': 0, 'video_id': '07092'}, {'bbox': [25, 0, 220, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 16, 'signer_id': 18, 'source': 'aslpro', 'split': 'test', 'url': 'http://www.aslpro.com/main/b/book_music.swf', 'variation_id': 0, 'video_id': '07093'}, {'bbox': [234, 17, 524, 414], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 17, 'signer_id': 36, 'source': 'startasl', 'split': 'train', 'url': 'https://s3-us-west-1.amazonaws.com/files.startasl.com/asldictionary/book.mp4', 'variation_id': 0, 'video_id': '07068'}, {'bbox': [22, 2, 231, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 18, 'signer_id': 18, 'source': 'aslpro', 'split': 'val', 'url': 'http://www.aslpro.com/main/b/book_photography.swf', 'variation_id': 0, 'video_id': '07094'}, {'bbox': [3, 2, 260, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 19, 'signer_id': 18, 'source': 'aslpro', 'split': 'test', 'url': 'http://www.aslpro.com/main/b/book_science.swf', 'variation_id': 0, 'video_id': '07095'}, {'bbox': [21, 2, 228, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 20, 'signer_id': 18, 'source': 'aslpro', 'split': 'val', 'url': 'http://www.aslpro.com/main/b/book_spelling.swf', 'variation_id': 0, 'video_id': '07096'}, {'bbox': [26, 3, 226, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 21, 'signer_id': 49, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book.swf', 'variation_id': 0, 'video_id': '07097'}, {'bbox': [131, 26, 526, 480], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 22, 'signer_id': 59, 'source': 'asldeafined', 'split': 'train', 'url': 'https://media.asldeafined.com/vocabulary/1466684225.687.mp4', 'variation_id': 0, 'video_id': '07070'}, {'bbox': [21, 3, 231, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 23, 'signer_id': 18, 'source': 'aslpro', 'split': 'val', 'url': 'http://www.aslpro.com/main/b/book_trigonometry.swf', 'variation_id': 0, 'video_id': '07098'}, {'bbox': [162, 54, 528, 400], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 24, 'signer_id': 12, 'source': 'aslsearch', 'split': 'val', 'url': 'http://www.aslsearch.com/signs/videos/book.mp4', 'variation_id': 0, 'video_id': '07099'}, {'bbox': [70, 0, 268, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 25, 'signer_id': 14, 'source': 'handspeak', 'split': 'train', 'url': 'https://www.handspeak.com/word/b/book2.mp4', 'variation_id': 0, 'video_id': '07071'}, {'bbox': [75, 0, 270, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 26, 'signer_id': 14, 'source': 'handspeak', 'split': 'test', 'url': 'https://www.handspeak.com/word/b/book.mp4', 'variation_id': 0, 'video_id': '07072'}, {'bbox': [64, 0, 273, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 27, 'signer_id': 14, 'source': 'handspeak', 'split': 'train', 'url': 'https://www.handspeak.com/word/b/books-pile.mp4', 'variation_id': 0, 'video_id': '07073'}, {'bbox': [128, 20, 383, 360], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 28, 'signer_id': 109, 'source': 'asllex', 'split': 'train', 'url': 'https://youtu.be/3-GrhsVs830', 'variation_id': 0, 'video_id': '67424'}, {'bbox': [82, 11, 212, 192], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 29, 'signer_id': 11, 'source': 'signingsavvy', 'split': 'train', 'url': 'https://www.signingsavvy.com/signs/mp4/14/14326.mp4', 'variation_id': 0, 'video_id': '07074'}, {'bbox': [386, 48, 942, 720], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 30, 'signer_id': 5, 'source': 'aslu', 'split': 'train', 'url': 'https://www.youtube.com/watch?v=Kwvw-K6GYW8', 'variation_id': 0, 'video_id': '07075'}, {'bbox': [362, 43, 945, 720], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 31, 'signer_id': 5, 'source': 'aslu', 'split': 'train', 'url': 'https://www.youtube.com/watch?v=XjWSfh50kAU', 'variation_id': 0, 'video_id': '07076'}, {'bbox': [25, 4, 223, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 32, 'signer_id': 49, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_accounting.swf', 'variation_id': 0, 'video_id': '07077'}, {'bbox': [28, 4, 226, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 33, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_algebra.swf', 'variation_id': 0, 'video_id': '07078'}, {'bbox': [15, 2, 238, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 34, 'signer_id': 49, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_anatomy.swf', 'variation_id': 0, 'video_id': '07079'}, {'bbox': [28, 2, 226, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 35, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_art_history.swf', 'variation_id': 0, 'video_id': '07080'}, {'bbox': [23, 2, 226, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 36, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_art.swf', 'variation_id': 0, 'video_id': '07081'}, {'bbox': [30, 3, 228, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 37, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_business.swf', 'variation_id': 0, 'video_id': '07082'}, {'bbox': [0, 3, 260, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 38, 'signer_id': 18, 'source': 'aslpro', 'split': 'val', 'url': 'http://www.aslpro.com/main/b/book_chemistry.swf', 'variation_id': 0, 'video_id': '07083'}, {'bbox': [29, 4, 225, 240], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 39, 'signer_id': 18, 'source': 'aslpro', 'split': 'train', 'url': 'http://www.aslpro.com/main/b/book_coloring.swf', 'variation_id': 0, 'video_id': '07084'}]}]\n"
          ]
        }
      ],
      "source": [
        "with open(\"WLASL_v0.3.json\", \"r\") as f:\n",
        "  json_data = json.load(f)\n",
        "print(json_data[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-fSssQS_lZz"
      },
      "outputs": [],
      "source": [
        "#Define directories\n",
        "sign_video_data_dir =\"video_data\"\n",
        "train_dir =os.path.join(sign_video_data_dir, \"train\")\n",
        "val_dir =os.path.join(sign_video_data_dir, \"val\")\n",
        "test_dir = os.path.join(sign_video_data_dir, \"test\")\n",
        "\n",
        "if not os.path.exists(sign_video_data_dir):\n",
        "  os.mkdir(sign_video_data_dir)\n",
        "  os.mkdir(train_dir)\n",
        "  os.mkdir(val_dir)\n",
        "  os.mkdir(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqwscRlCAvO4",
        "outputId": "c8782a1f-2157-426a-b296-db225b8c6bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  train  val\n"
          ]
        }
      ],
      "source": [
        "!cd video_data/ && ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LXKMeWXCNtb",
        "outputId": "5809e04f-8ad3-4696-8d9a-9d7052b36719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bbox': [385, 37, 885, 720], 'fps': 25, 'frame_end': -1, 'frame_start': 1, 'instance_id': 0, 'signer_id': 118, 'source': 'aslbrick', 'split': 'train', 'url': 'http://aslbricks.org/New/ASL-Videos/book.mp4', 'variation_id': 0, 'video_id': '69241'}\n"
          ]
        }
      ],
      "source": [
        "for data in json_data[:10]:\n",
        "  for instances in data[\"instances\"]:\n",
        "    #print(instances['split'], instances['url'], data['gloss'])# instances['video_id'] )#['fps'])\n",
        "    #print(path)\n",
        "    path = os.path.join(sign_video_data_dir, f\"{instances['split']}/{data['gloss']}\")\n",
        "    print(instances)\n",
        "    break\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QLIPu8VjDkkR"
      },
      "outputs": [],
      "source": [
        "# Split Video\n",
        "import shutil\n",
        "for data in json_data:\n",
        "  for instances in data[\"instances\"]:\n",
        "    #print(instances['split'], instances['url'], data['gloss'])# instances['video_id'] )#['fps'])\n",
        "    path = os.path.join(sign_video_data_dir, f\"{instances['split']}/{data['gloss']}\")\n",
        "    video_path = f\"/content/drive/MyDrive/Tensorflow dev/real_time_sign_language/wasl_data/wlasl-complete/videos/{instances['video_id']}.mp4\"\n",
        "    try:\n",
        "      shutil.move(video_path, os.path.join(path, f\"{instances['video_id']}.mp4\"))\n",
        "    except:\n",
        "      pass\n",
        "      #print(video_path, os.path.join(path, f\"{instances['video_id']}.mp4\"))\n"
      ]
    }
  ]
}